{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0457fff9-1a2c-4cd0-ad57-aa33be36db36",
   "metadata": {},
   "source": [
    "# Lab Project: Deconstructing the Transformer\n",
    "\n",
    "Related video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "**\"Attention is All You Need\" â€“ From Scratch**\n",
    "\n",
    "- **Duration:** 2 Weeks\n",
    "   \n",
    "- **Tools:** Python, PyTorch (recommended) or TensorFlow/JAX\n",
    "   \n",
    "- **Dataset:** [\"Tiny Shakespeare\"](https://huggingface.co/datasets/karpathy/tiny_shakespeare) (Character-level text generation)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd464f-edb6-4bd4-b4da-ec985a291e9c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "The goal of this lab is not to use a pre-built library like Hugging Face `transformers` to fine-tune a model. Instead, you will implement the Transformer architecture **layer-by-layer** using basic tensor operations.\n",
    "\n",
    "By the end of this assignment, you will have a working **Decoder-Only Transformer** (a mini-GPT) capable of generating Shakespearean-style text. You will understand the exact flow of gradients through Self-Attention, Layer Normalization, and Residual Connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b020eee8-c02d-4078-8d86-f6f533654067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89973a5-7adb-45c6-99ed-053c73865516",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "161cd6db-9241-4e6b-ac5b-6ae973863c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11549ddd-c047-46a9-a6d2-21ab9d273a18",
   "metadata": {},
   "source": [
    "## 2. The Dataset & Preprocessing\n",
    "\n",
    "We will use the \"Tiny Shakespeare\" dataset. It is small, trains quickly on a CPU/low-end GPU, and allows for immediate visual verification (i.e., does the output look like English?).\n",
    "\n",
    "**Task 0: Setup**\n",
    "\n",
    "1. Download `input.txt` (Tiny Shakespeare).\n",
    "   \n",
    "2. Create a tokenizer: Build a dictionary mapping unique characters to integers (encoding) and integers back to characters (decoding).\n",
    "   \n",
    "3. Create a PyTorch `Dataset` or data loader that serves batches of context blocks (e.g., block size of 32 or 64 characters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e0d3d036-5ed3-4c1d-bf14-5958ba2832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file input.txt from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt...\n"
     ]
    }
   ],
   "source": [
    "input_file = 'input.txt'\n",
    "file_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "print(f'Downloading file {input_file} from {file_url}...')\n",
    "with open(input_file,'w') as f:\n",
    "    f.write(requests.get(file_url).text)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5b90c88b-afa5-4657-9eef-462d980767a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of symbols : 65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Total number of symbols : {len(chars)}')\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "eaa7f6f3-0841-45f8-acf4-a54536e20d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
      " {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[x] for x in s]\n",
    "decode = lambda i: ''.join([itos[x] for x in i])\n",
    "\n",
    "print(stoi,'\\n',itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "36e83841-488a-44ac-9658-0fa8e719fbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y= x.to(device), y.to(device)\n",
    "    return (x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb85a6-7531-4a43-a8ba-ac74efb198d5",
   "metadata": {},
   "source": [
    "## 3. Implementation Milestones\n",
    "\n",
    "You must implement the architecture in an object-oriented fashion. Do not use `torch.nn.Transformer` or `torch.nn.MultiheadAttention`. You must build these classes yourself using `torch.nn.Linear`, `torch.matmul`, etc.\n",
    "\n",
    "### Part I: Positional Embeddings\n",
    "\n",
    "Transformers process tokens in parallel, meaning they have no inherent sense of order. You must inject this information.\n",
    "\n",
    "- **Requirement:** Implement the sinusoidal positional encoding as described in the original paper _Attention is All You Need_.\n",
    "   \n",
    "- Formula:\n",
    "   \n",
    "    $$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "   \n",
    "    $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "   \n",
    "- **Deliverable:** A class `PositionalEncoding(d_model, max_len)` and a plot visualizing the embeddings (heatmap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1f9576a8-1203-45c9-9068-a70952f6056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        return x + self.pe[:T, :]\n",
    "\n",
    "    def plot_heatmap(self, length=100):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        pos_embed = copy.deepcopy(self.pe[:length, :])\n",
    "        plt.imshow(pos_embed.cpu().numpy(), cmap='viridis')\n",
    "        plt.title('Sinusoidal Positional Embeddings')\n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Position (Time Step)')\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d4e0a-c3f0-4825-831f-081916a0386f",
   "metadata": {},
   "source": [
    "### Part II: Scaled Dot-Product Attention (The Core)\n",
    "\n",
    "This is the mathematical engine of the Transformer.\n",
    "\n",
    "- Requirement: Implement a function that takes Query ($Q$), Key ($K$), and Value ($V$) matrices and computes:\n",
    "   \n",
    "    $$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "   \n",
    "- **Critical Detail:** You must implement a **Mask**. Since this is a decoder-only model for text generation, the model cannot \"see the future.\" You must apply a lower-triangular mask (setting upper values to $-\\infty$) before the softmax so that position $t$ can only attend to positions $0$ through $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "06bdc130-4370-4458-a62c-2d7482ccb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288c977-a440-44b6-9159-ff11b881376c",
   "metadata": {},
   "source": [
    "### Part III: Multi-Head Attention (MHA)\n",
    "\n",
    "Single-head attention captures one type of relationship. MHA allows the model to focus on different positions jointly from different representation subspaces.\n",
    "\n",
    "- **Requirement:** Create a `MultiHeadAttention` class.\n",
    "   \n",
    "    1. Linear projections for $Q$, $K$, and $V$.\n",
    "       \n",
    "    2. Split the heads (reshape the tensors).\n",
    "       \n",
    "    3. Apply Scaled Dot-Product Attention (from Part II).\n",
    "       \n",
    "    4. Concatenate heads and apply a final linear projection.\n",
    "       \n",
    "- **Code Hint:** Be careful with tensor shapes.\n",
    "   \n",
    "    - Input: `(Batch, Time, Channels)`\n",
    "       \n",
    "    - Reshape to: `(Batch, Time, Heads, Head_Dim)`\n",
    "       \n",
    "    - Transpose for matmul: `(Batch, Heads, Time, Head_Dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "41c8401e-552b-4d87-9aa2-52daae6c8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4e27c-bb77-4c3b-b566-d92d9689cca8",
   "metadata": {},
   "source": [
    "### Part IV: The Transformer Block\n",
    "\n",
    "Assemble the components into a repeatable layer.\n",
    "\n",
    "- **Requirement:** Create a `Block` class containing:\n",
    "   \n",
    "    1. **Layer Normalization:** Applied _before_ the sub-layer (Pre-Norm formulation is generally more stable than Post-Norm).\n",
    "       \n",
    "    2. **Multi-Head Attention:** Your class from Part III.\n",
    "       \n",
    "    3. **Feed-Forward Network:** A simple MLP expanding the dimension by 4x (e.g., `d_model` -> `4*d_model` -> `d_model`) with ReLU or GeLU activation.\n",
    "       \n",
    "    4. **Residual Connections:** $x + Sublayer(Norm(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e8805df1-5df8-41f3-b848-f04e8792d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4844e569-8aff-45f4-84f6-687fac9b85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3199fba-4126-43f5-ae49-9dfd127a851e",
   "metadata": {},
   "source": [
    "## 4. Assembly and Training\n",
    "\n",
    "### The Model\n",
    "\n",
    "Create a `GPTLanguageModel` class that stacks:\n",
    "\n",
    "1. Token Embeddings + Positional Encodings.\n",
    "   \n",
    "2. $N$ layers of your `Block` (Try $N=4$ to $6$).\n",
    "   \n",
    "3. Final Layer Norm.\n",
    "   \n",
    "4. Final Linear Head (projecting to vocabulary size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9f427a66-4cec-4898-90eb-5e896b1680a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        #self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.position_embedding_table = PositionalEncoding(n_embd, max_len=block_size)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        x = self.position_embedding_table(tok_emb) #torch.arange(T, device=device)) # (T,C)\n",
    "        #x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focs only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B,C)\n",
    "            # apply softmax to the get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf4a9e-f602-45b3-9b2c-52eece2e7d64",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "- **Hyperparameters:**\n",
    "   \n",
    "    - Batch size: 32 or 64\n",
    "       \n",
    "    - Block size (context length): 128 or 256\n",
    "       \n",
    "    - Embedding dimension ($d_{model}$): 384\n",
    "       \n",
    "    - Heads: 6\n",
    "       \n",
    "    - Learning Rate: 3e-4 (use AdamW optimizer)\n",
    "       \n",
    "- **Metric:** Calculate Cross Entropy Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b48d2854-5a48-4a18-ab0c-f70ae40735e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model ...\n",
      "Setting up optimizer...\n",
      "Starting training ...\n",
      "Estimating loss...\n",
      "step 0: train loss 4.5226, val loss 4.5125\n",
      "Estimating loss...\n",
      "step 500: train loss 2.6253, val loss 2.6245\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 4\n",
    "max_iters = 1000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "dropout = 0.2\n",
    "\n",
    "print('Creating Model ...')\n",
    "model = GPTLanguageModel()\n",
    "model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "print('Setting up optimizer...')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Starting training ...')\n",
    "for iter in range(max_iters):\n",
    "    # evaluuate loss on train and val\n",
    "    if iter % eval_interval == 0:# | iter == max_iters-1:\n",
    "        print('Estimating loss...')\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    #sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss =model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdeb66-4f86-4718-836f-34e8fa55b858",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e128ae-a079-4f10-95a3-7ec49b154c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669eb168-271e-42e2-80db-362609df7991",
   "metadata": {},
   "source": [
    "### Generation Function\n",
    "\n",
    "Implement a `generate` function.\n",
    "\n",
    "- Take a starting context (e.g., a single character).\n",
    "   \n",
    "- Pass through the model to get logits.\n",
    "   \n",
    "- Apply Softmax to get probabilities.\n",
    "   \n",
    "- Sample from the distribution (`torch.multinomial`).\n",
    "   \n",
    "- Append the new character and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0ae376ac-6ff4-4a2a-8f32-2e718b4a0a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Lroerd has moadWes to redeaim, grucr, dastespco bout bell to f is ndods thel, arle?\n",
      "\n",
      "\n",
      "\n",
      "Whow heaes,\n",
      "\n",
      "Ce it apitng,.\n",
      "MEKE AUSHEOM:\n",
      "ForTils\n",
      "Boarbreneld tar, ist let devo ase ar beatd JUES:\n",
      "Ankby;\n",
      "\n",
      "Teve mys!\n",
      "Weade marloou gior mon-foy nor; ee:\n",
      "Whast rue a wiAs\n",
      "Dade Ofoour, thuillde;\n",
      "\n",
      "Ren:\n",
      "Why?\n",
      "\n",
      "\n",
      "WAnd willd; youhee wis lastideslen then. no lof aloud hanedof hid Yo tha bee keake qun g hend \n",
      "lyompsaislllwees, slosstiriel bee,\n",
      "Thakeyer.\n",
      "\n",
      "\n",
      "y Cavom ise alt ingis Yolve onadRuged our.\n",
      "\n",
      "WArus deas, sfand an\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a643e-c63e-4ba8-b9f5-e733eeeb22c8",
   "metadata": {},
   "source": [
    "### Analysis Questions (Include in Report)\n",
    "\n",
    "1. **Scaling:** Why do we divide by $\\sqrt{d_k}$ in the attention formula? What happens to the Softmax gradients if we don't?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fb239-5e36-4499-a9c6-f61295667eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceed3ea3-ef6d-44a6-ab8c-c74d06ab51a0",
   "metadata": {},
   "source": [
    "2. **Positional Encoding:** Why do we add positional encodings to the embeddings rather than concatenating them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "22cc7af6-e9b4-40b0-bbbc-be9c2bd590fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.position_embedding_table.plot_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b6e4-57dd-485c-ae9d-ec1a9954e8ba",
   "metadata": {},
   "source": [
    "3. **Complexity:** What is the Big-O time complexity of the Self-Attention mechanism with respect to the sequence length $T$? Why is this a problem for very long texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214800b2-841c-485d-bfb5-9c55be23e8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c03454-fb9a-458d-af74-5e74f3890cdf",
   "metadata": {},
   "source": [
    "## 6. Starter Code Snippet (Helper)\n",
    "\n",
    "Here is the signature for your Multi-Head Attention to get you started:\n",
    "\n",
    "Python\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        # You need to register the mask as a buffer so it's not treated as a parameter\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "       \n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B: Batch, T: Time (Sequence Length), C: Channels (Embed size)\n",
    "        B, T, C = x.shape\n",
    "       \n",
    "        # Implementation goes here...\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d198d-d08f-4531-94e9-4333d62dae18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
