{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0457fff9-1a2c-4cd0-ad57-aa33be36db36",
   "metadata": {},
   "source": [
    "# Lab Project: Deconstructing the Transformer\n",
    "\n",
    "Related video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "**\"Attention is All You Need\" â€“ From Scratch**\n",
    "\n",
    "- **Duration:** 2 Weeks\n",
    "   \n",
    "- **Tools:** Python, PyTorch (recommended) or TensorFlow/JAX\n",
    "   \n",
    "- **Dataset:** [\"Tiny Shakespeare\"](https://huggingface.co/datasets/karpathy/tiny_shakespeare) (Character-level text generation)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd464f-edb6-4bd4-b4da-ec985a291e9c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "The goal of this lab is not to use a pre-built library like Hugging Face `transformers` to fine-tune a model. Instead, you will implement the Transformer architecture **layer-by-layer** using basic tensor operations.\n",
    "\n",
    "By the end of this assignment, you will have a working **Decoder-Only Transformer** (a mini-GPT) capable of generating Shakespearean-style text. You will understand the exact flow of gradients through Self-Attention, Layer Normalization, and Residual Connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b020eee8-c02d-4078-8d86-f6f533654067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11549ddd-c047-46a9-a6d2-21ab9d273a18",
   "metadata": {},
   "source": [
    "## 2. The Dataset & Preprocessing\n",
    "\n",
    "We will use the \"Tiny Shakespeare\" dataset. It is small, trains quickly on a CPU/low-end GPU, and allows for immediate visual verification (i.e., does the output look like English?).\n",
    "\n",
    "**Task 0: Setup**\n",
    "\n",
    "1. Download `input.txt` (Tiny Shakespeare).\n",
    "   \n",
    "2. Create a tokenizer: Build a dictionary mapping unique characters to integers (encoding) and integers back to characters (decoding).\n",
    "   \n",
    "3. Create a PyTorch `Dataset` or data loader that serves batches of context blocks (e.g., block size of 32 or 64 characters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de4f396b-d796-4fed-8b2d-c8ebc60956e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file input.txt from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt...\n"
     ]
    }
   ],
   "source": [
    "input_file = 'input.txt'\n",
    "file_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "print(f'Downloading file {input_file} from {file_url}...')\n",
    "with open(input_file,'w') as f:\n",
    "    f.write(requests.get(file_url).text)\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da1f0870-f08e-45ea-8755-a71116ed1559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'Data length: {len(data)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb85a6-7531-4a43-a8ba-ac74efb198d5",
   "metadata": {},
   "source": [
    "## 3. Implementation Milestones\n",
    "\n",
    "You must implement the architecture in an object-oriented fashion. Do not use `torch.nn.Transformer` or `torch.nn.MultiheadAttention`. You must build these classes yourself using `torch.nn.Linear`, `torch.matmul`, etc.\n",
    "\n",
    "### Part I: Positional Embeddings\n",
    "\n",
    "Transformers process tokens in parallel, meaning they have no inherent sense of order. You must inject this information.\n",
    "\n",
    "- **Requirement:** Implement the sinusoidal positional encoding as described in the original paper _Attention is All You Need_.\n",
    "   \n",
    "- Formula:\n",
    "   \n",
    "    $$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "   \n",
    "    $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "   \n",
    "- **Deliverable:** A class `PositionalEncoding(d_model, max_len)` and a plot visualizing the embeddings (heatmap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9576a8-1203-45c9-9068-a70952f6056b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b9d4e0a-c3f0-4825-831f-081916a0386f",
   "metadata": {},
   "source": [
    "### Part II: Scaled Dot-Product Attention (The Core)\n",
    "\n",
    "This is the mathematical engine of the Transformer.\n",
    "\n",
    "- Requirement: Implement a function that takes Query ($Q$), Key ($K$), and Value ($V$) matrices and computes:\n",
    "   \n",
    "    $$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "   \n",
    "- **Critical Detail:** You must implement a **Mask**. Since this is a decoder-only model for text generation, the model cannot \"see the future.\" You must apply a lower-triangular mask (setting upper values to $-\\infty$) before the softmax so that position $t$ can only attend to positions $0$ through $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7eb370-1b51-4994-86b3-b26a247faf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7288c977-a440-44b6-9159-ff11b881376c",
   "metadata": {},
   "source": [
    "### Part III: Multi-Head Attention (MHA)\n",
    "\n",
    "Single-head attention captures one type of relationship. MHA allows the model to focus on different positions jointly from different representation subspaces.\n",
    "\n",
    "- **Requirement:** Create a `MultiHeadAttention` class.\n",
    "   \n",
    "    1. Linear projections for $Q$, $K$, and $V$.\n",
    "       \n",
    "    2. Split the heads (reshape the tensors).\n",
    "       \n",
    "    3. Apply Scaled Dot-Product Attention (from Part II).\n",
    "       \n",
    "    4. Concatenate heads and apply a final linear projection.\n",
    "       \n",
    "- **Code Hint:** Be careful with tensor shapes.\n",
    "   \n",
    "    - Input: `(Batch, Time, Channels)`\n",
    "       \n",
    "    - Reshape to: `(Batch, Time, Heads, Head_Dim)`\n",
    "       \n",
    "    - Transpose for matmul: `(Batch, Heads, Time, Head_Dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f742cc-b272-4d46-855d-1d2de160eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d4e27c-bb77-4c3b-b566-d92d9689cca8",
   "metadata": {},
   "source": [
    "### Part IV: The Transformer Block\n",
    "\n",
    "Assemble the components into a repeatable layer.\n",
    "\n",
    "- **Requirement:** Create a `Block` class containing:\n",
    "   \n",
    "    1. **Layer Normalization:** Applied _before_ the sub-layer (Pre-Norm formulation is generally more stable than Post-Norm).\n",
    "       \n",
    "    2. **Multi-Head Attention:** Your class from Part III.\n",
    "       \n",
    "    3. **Feed-Forward Network:** A simple MLP expanding the dimension by 4x (e.g., `d_model` -> `4*d_model` -> `d_model`) with ReLU or GeLU activation.\n",
    "       \n",
    "    4. **Residual Connections:** $x + Sublayer(Norm(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7202b-29e4-4ac6-9825-63f5a9d1935a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3199fba-4126-43f5-ae49-9dfd127a851e",
   "metadata": {},
   "source": [
    "## 4. Assembly and Training\n",
    "\n",
    "### The Model\n",
    "\n",
    "Create a `GPTLanguageModel` class that stacks:\n",
    "\n",
    "1. Token Embeddings + Positional Encodings.\n",
    "   \n",
    "2. $N$ layers of your `Block` (Try $N=4$ to $6$).\n",
    "   \n",
    "3. Final Layer Norm.\n",
    "   \n",
    "4. Final Linear Head (projecting to vocabulary size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259f8cc-8892-47d2-9f0a-c5e15c433122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aadf4a9e-f602-45b3-9b2c-52eece2e7d64",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "- **Hyperparameters:**\n",
    "   \n",
    "    - Batch size: 32 or 64\n",
    "       \n",
    "    - Block size (context length): 128 or 256\n",
    "       \n",
    "    - Embedding dimension ($d_{model}$): 384\n",
    "       \n",
    "    - Heads: 6\n",
    "       \n",
    "    - Learning Rate: 3e-4 (use AdamW optimizer)\n",
    "       \n",
    "- **Metric:** Calculate Cross Entropy Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853fe3e0-c406-44ae-902b-23442dc0f040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "669eb168-271e-42e2-80db-362609df7991",
   "metadata": {},
   "source": [
    "### Generation Function\n",
    "\n",
    "Implement a `generate` function.\n",
    "\n",
    "- Take a starting context (e.g., a single character).\n",
    "   \n",
    "- Pass through the model to get logits.\n",
    "   \n",
    "- Apply Softmax to get probabilities.\n",
    "   \n",
    "- Sample from the distribution (`torch.multinomial`).\n",
    "   \n",
    "- Append the new character and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae376ac-6ff4-4a2a-8f32-2e718b4a0a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24a643e-c63e-4ba8-b9f5-e733eeeb22c8",
   "metadata": {},
   "source": [
    "### Analysis Questions (Include in Report)\n",
    "\n",
    "1. **Scaling:** Why do we divide by $\\sqrt{d_k}$ in the attention formula? What happens to the Softmax gradients if we don't?\n",
    "   \n",
    "2. **Positional Encoding:** Why do we add positional encodings to the embeddings rather than concatenating them?\n",
    "   \n",
    "3. **Complexity:** What is the Big-O time complexity of the Self-Attention mechanism with respect to the sequence length $T$? Why is this a problem for very long texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c03454-fb9a-458d-af74-5e74f3890cdf",
   "metadata": {},
   "source": [
    "## 6. Starter Code Snippet (Helper)\n",
    "\n",
    "Here is the signature for your Multi-Head Attention to get you started:\n",
    "\n",
    "Python\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size * num_heads, bias=False)\n",
    "        # You need to register the mask as a buffer so it's not treated as a parameter\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "       \n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B: Batch, T: Time (Sequence Length), C: Channels (Embed size)\n",
    "        B, T, C = x.shape\n",
    "       \n",
    "        # Implementation goes here...\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d198d-d08f-4531-94e9-4333d62dae18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
